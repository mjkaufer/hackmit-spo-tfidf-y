{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from typing import List\n",
    "\n",
    "# boring string formatting stuff ;)\n",
    "punctuation_set = set(string.punctuation)\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(ch for ch in s if ch not in punctuation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an array of vectors, with each vector representing a given document\n",
    "# also returns corresponding words per row\n",
    "def create_tfidf_for_documents(documents: List[str]):\n",
    "\n",
    "    # array of array of words in a given doc\n",
    "    # make sure we sanitize words by removing uppercase & punctuation\n",
    "    doc_word_lists = [strip_punctuation(doc.lower()).split(' ') for doc in documents]\n",
    "\n",
    "    # gets us unique words from within our docs\n",
    "    unique_words = [*{word for doc_word_list in doc_word_lists for word in doc_word_list}]\n",
    "    word_to_index = {word: index for index, word in enumerate(unique_words)}\n",
    "\n",
    "    # want a row for each word, col for each doc\n",
    "    num_words = len(unique_words)\n",
    "    num_docs = len(documents)\n",
    "\n",
    "    tf_mat = np.zeros((num_words, num_docs))\n",
    "    # idf is global across documents\n",
    "    idf_mat = np.zeros(num_words)\n",
    "\n",
    "    for doc_index, doc_word_list in enumerate(doc_word_lists):\n",
    "        seen_words = set()\n",
    "\n",
    "        # count word instances for tf\n",
    "        for word in doc_word_list:\n",
    "            word_index = word_to_index[word]\n",
    "            tf_mat[word_index][doc_index] += 1\n",
    "\n",
    "            # don't want to double count IDF in the same document\n",
    "            if word not in seen_words:\n",
    "                idf_mat[word_index] += 1\n",
    "                seen_words.add(word)\n",
    "\n",
    "        # normalize TF by word count\n",
    "        tf_mat[:, doc_index] /= len(doc_word_list)\n",
    "\n",
    "    # calculate log of idf once we've calculated\n",
    "    for word_index, idf in enumerate(idf_mat):\n",
    "        idf_mat[word_index] = np.log10(num_docs / idf)\n",
    "\n",
    "\n",
    "    # row-wise multiplication within idf_mat!\n",
    "    tfidf_mat = tf_mat * idf_mat[:, np.newaxis]\n",
    "\n",
    "    return (tfidf_mat, unique_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"My dog is named Fido. Fido likes to bark and runs fast.\",\n",
    "    \"My cat is named Puru. Puru likes to purr and is fluffy.\",\n",
    "    \"My puppy is cute and can bark loudly\",\n",
    "]\n",
    "\n",
    "mat, mat_words = create_tfidf_for_documents(docs)\n",
    "\n",
    "for (index, word) in enumerate(mat_words):\n",
    "    print(word, \"\\t\", mat[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dog vs puppy is {}\".format(mat[:,0].dot(mat[:,2])))\n",
    "print(\"cat vs puppy is {}\".format(mat[:,1].dot(mat[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrape import SpotifyScraper\n",
    "\n",
    "# connect to spotify API\n",
    "scraper = SpotifyScraper()\n",
    "scraper.authenticate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is vampire weekend, but you can find other album IDs by\n",
    "# just going to spotify, copying the share link, and extracting the deets from there!\n",
    "artistId = '5BvJzeQpmsdsFp4HGUYUEx'\n",
    "discography, trackToName, albumToName = scraper.getArtistDiscography(artistId=artistId)\n",
    "print(discography)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "readable_discography = {albumToName[albumId]:[trackToName[track] for track in tracks] for (albumId, tracks) in discography.items()}\n",
    "print(json.dumps(readable_discography, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runningPlaylistResults = {}\n",
    "\n",
    "# set of all of our tracks!\n",
    "# used to figure out if a playlist has relevant songs\n",
    "tracks = {song for songs in discography.values() for song in songs}\n",
    "try:\n",
    "    for i in range(4):\n",
    "        for genre in [\"indie\", \"indie rock\", \"indie pop\", \"2000s indie\", \"indie punk\"]:\n",
    "            # this will query for playlists with names similar to the above phrases\n",
    "            # ensures that each playlist has at least two tracks from our list\n",
    "            # and adds the results to runningPlaylistResults\n",
    "            runningPlaylistResults = scraper.playlistQuery(genre, tracks, runningPlaylistResults)\n",
    "except:\n",
    "    print(\"Uh oh, oopsie while scraping, probably got rate limited!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print({k:{**v, 'tracks': [trackToName[t] for t in v['tracks']]} for k,v in runningPlaylistResults.items()})\n",
    "print(json.dumps([[trackToName[t] for t in v['tracks']] for k,v in runningPlaylistResults.items()], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
